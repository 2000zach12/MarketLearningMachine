{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb3c798f2f5b415e",
   "metadata": {},
   "source": [
    "## Import Libraries Load data and clean"
   ]
  },
  {
   "cell_type": "code",
   "id": "809d865f1fdf0c08",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest Regression\n",
    "# Intall Libraries\n",
    "\n",
    "#install numpy\n",
    "#install pandas\n",
    "#install matplotlib\n",
    "#install seaborn\n",
    "#install sklearn\n",
    "#install statsmodels\n",
    "#install openpyxl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "sns.set()  #if you want to use seaborn themes with matplotlib functions\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "dfUnclean=pd.read_excel('F 1021 to 1025.xlsx')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee441902065e43a5",
   "metadata": {},
   "source": [
    "#check that data looks allright\n",
    "dfUnclean.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53240b7995d63876",
   "metadata": {},
   "source": [
    "# There are dividends paid and as such there are more dates and \"open values\" but in reality those are the dividend amounts\n",
    "dfUnclean.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd2bd9c240180944",
   "metadata": {},
   "source": [
    "# Show sample rows where any column has 'Dividend'\n",
    "div_check = dfUnclean[dfUnclean.apply(lambda row: row.astype(str).str.contains(\"Dividend\", case=False, na=False).any(), axis=1)]\n",
    "\n",
    "print(div_check)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81e73ecc415f88f6",
   "metadata": {},
   "source": [
    "# Identify dividend rows\n",
    "div_mask = dfUnclean[\"Open\"].astype(str).str.contains(\"Dividend\", case=False, na=False)\n",
    "\n",
    "div_rows = dfUnclean[div_mask]\n",
    "price_rows = dfUnclean[~div_mask]\n",
    "\n",
    "# Extract dividend amount\n",
    "div_rows[\"Dividend\"] = div_rows[\"Open\"].astype(str).str.extract(r\"([0-9]*\\.?[0-9]+)\").astype(float)\n",
    "div_rows = div_rows[[\"Date\", \"Dividend\"]]\n",
    "\n",
    "# Convert Date to datet\n",
    "div_rows[\"Date\"] = pd.to_datetime(div_rows[\"Date\"])\n",
    "price_rows[\"Date\"] = pd.to_datetime(price_rows[\"Date\"])\n",
    "\n",
    "dfUnclean = price_rows.merge(div_rows, on=\"Date\", how=\"left\")\n",
    "\n",
    "if(dfUnclean[\"Dividend\"].isna().any()==True):\n",
    "    dfUnclean[\"Dividend\"] = dfUnclean[\"Dividend\"].fillna(0)\n",
    "\n",
    "#convert volume to int\n",
    "dfUnclean[\"Volume\"] = dfUnclean[\"Volume\"].astype(int)\n",
    "\n",
    "df = dfUnclean\n",
    "\n",
    "print(df.iloc[50])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13ff2c6afa943eb8",
   "metadata": {},
   "source": [
    "#Check for missing data\n",
    "df.info()\n",
    "df.isnull().sum()/len(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e50b426f847c608a",
   "metadata": {},
   "source": [
    "#check for duplicates\n",
    "sum(df.duplicated())\n",
    "df.nunique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6c688a3f098f3be4",
   "metadata": {},
   "source": [
    "## Adding Autoregressive Varriables"
   ]
  },
  {
   "cell_type": "code",
   "id": "8d73e511da557c60",
   "metadata": {},
   "source": [
    "def create_lagged_features(df, lags, cols):\n",
    "    for col in cols:\n",
    "        for i in range(1,lags+1):\n",
    "            df[f'{col}_lag_{i}'] = df[col].shift(i)\n",
    "\n",
    "    return df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68eea41efce1de5f",
   "metadata": {},
   "source": [
    "dfCopy =df.copy()\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "650533217246dc52",
   "metadata": {},
   "source": [
    "lag_features = ['Volume', 'Open', 'Close']\n",
    "create_lagged_features(df, lags=2, cols=lag_features)\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c080fd56feda9195",
   "metadata": {},
   "source": [
    "df.head"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5f4b8d97e61c01bb",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "id": "a9decd6c4568696f",
   "metadata": {},
   "source": [
    "target = \"Close\"\n",
    "\n",
    "# All columns that contain \"_lag_\" become features\n",
    "Y = df[target]\n",
    "X = df[[c for c in df.columns if \"_lag_\" in c]+ [c for c in df.columns if c not in lag_features and c not in [\"Date\", \"Adj Close\",\"High\",\"Low\", target]]]\n",
    "\n",
    "#20% of data to test\n",
    "#Double check that its the newest data that is being used for the test\n",
    "split = int(len(df) * 0.2)\n",
    "\n",
    "X_train, X_test = X.iloc[split:], X.iloc[:split]\n",
    "Y_train, Y_test = Y.iloc[split:], Y.iloc[:split]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9d2e8c5cc7b5a87",
   "metadata": {},
   "source": [
    "#Check that Data was split right\n",
    "print(f\"X_train Length {len(X_train)}\")\n",
    "print(f\"X_test Length {len(X_test)}\")\n",
    "print(f\"Y_train Length {len(Y_train)}\")\n",
    "print(f\"Y_test Length {len(Y_test)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "949de93c7b8d2b51",
   "metadata": {},
   "source": [
    "## Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "d649ee615173836b",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=50,\n",
    "    random_state=25565\n",
    ")\n",
    "\n",
    "model.fit(X_train, Y_train)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "864ccb64c51c1b0d",
   "metadata": {},
   "source": [
    "Y_pred_train = model.predict(X_train)\n",
    "Y_pred_test = model.predict(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aeebb0e8c5f29f28",
   "metadata": {},
   "source": [
    "## Test the models accuracy"
   ]
  },
  {
   "cell_type": "code",
   "id": "2dc8e9a8b986197d",
   "metadata": {},
   "source": [
    "def evaluate(Y_true, Y_pred):\n",
    "    mae = mean_absolute_error(Y_true, Y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_true, Y_pred))\n",
    "    r2 = r2_score(Y_true, Y_pred)\n",
    "\n",
    "    return mae, rmse, r2\n",
    "\n",
    "train_mae, train_rmse, train_r2 = evaluate(Y_train, Y_pred_train)\n",
    "test_mae, test_rmse, test_r2 = evaluate(Y_test, Y_pred_test)\n",
    "\n",
    "print(\"TRAINING PERFORMANCE:\")\n",
    "print(f\"MAE:  {train_mae:.4f}\")\n",
    "print(f\"RMSE: {train_rmse:.4f}\")\n",
    "print(f\"R²:   {train_r2:.4f}\")\n",
    "\n",
    "print(\"\\nTEST PERFORMANCE:\")\n",
    "print(f\"MAE:  {test_mae:.4f}\")\n",
    "print(f\"RMSE: {test_rmse:.4f}\")\n",
    "print(f\"R²:   {test_r2:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d3c7c406d4d90b4",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(Y_test.values, label=\"Actual\")\n",
    "plt.plot(Y_pred_test, label=\"Predicted\")\n",
    "plt.title(\"Actual vs Predicted (Test Set)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81c4f8d77cb14aca",
   "metadata": {},
   "source": [
    "importance = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "importance.sort_values(ascending=False).head(20)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "767d2e09362b62d7",
   "metadata": {},
   "source": [
    "importance.sort_values().tail(20).plot(kind='barh', figsize=(8,6))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "54997014a578f293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:54:48.584550Z",
     "start_time": "2025-11-29T18:54:48.583072Z"
    }
   },
   "source": [
    "## Walk forward multi-horizon testing 1,5,10,20,30,60,90 Days into the future"
   ]
  },
  {
   "cell_type": "code",
   "id": "b77011e1dbee5e1c",
   "metadata": {},
   "source": [
    "def walk_forward_horizon_test(model, X, Y,horizons):\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for h in horizons:\n",
    "\n",
    "        preds = []\n",
    "        actuals = []\n",
    "\n",
    "        for i in range(len(X) - h):\n",
    "            # Predict based on the feature row i\n",
    "            pred = model.predict([X.iloc[i].values])[0]\n",
    "            actual = Y.iloc[i + h]\n",
    "\n",
    "            preds.append(pred)\n",
    "            actuals.append(actual)\n",
    "\n",
    "        mae = mean_absolute_error(actuals, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(actuals, preds))\n",
    "        r2 = r2_score(actuals, preds)\n",
    "\n",
    "        results[h] = {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac02face29a0d1f0",
   "metadata": {},
   "source": [
    "horizons =[1,5]#,10,20,30,60,90]\n",
    "results = walk_forward_horizon_test(model, X_test, Y_test, horizons)\n",
    "\n",
    "for h, metrics in results.items():\n",
    "    print(f\"{h}-DAY FORECAST:\")\n",
    "    print(f\"MAE:  {metrics['MAE']:.4f}\")\n",
    "    print(f\"RMSE: {metrics['RMSE']:.4f}\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5e63a9f7e720eac0",
   "metadata": {},
   "source": [
    "def evaluate_multi_step_forecast(model, X_test, Y_test, max_horizon=10):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for horizon in range(1, max_horizon + 1):\n",
    "\n",
    "        preds = []\n",
    "        actuals = []\n",
    "\n",
    "        # For each starting point, iteratively forecast ahead\n",
    "        for i in range(len(X_test) - horizon):\n",
    "\n",
    "            X_input = X_test.iloc[i].copy()\n",
    "\n",
    "            # Iteratively predict up to the chosen horizon\n",
    "            for step in range(horizon):\n",
    "                pred = model.predict(X_input.values.reshape(1, -1))[0]\n",
    "\n",
    "                # shift autoregressive variables\n",
    "                if \"Close_lag_1\" in X_input.index:\n",
    "                    X_input[\"Close_lag_1\"] = pred\n",
    "                if \"Open_lag_1\" in X_input.index:\n",
    "                    X_input[\"Open_lag_1\"] = pred\n",
    "                if \"Volume_lag_1\" in X_input.index:\n",
    "                    X_input[\"Volume_lag_1\"] = pred\n",
    "\n",
    "            preds.append(pred)\n",
    "            actuals.append(Y_test.iloc[i + horizon])\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        actuals = np.array(actuals)\n",
    "\n",
    "        mae = mean_absolute_error(actuals, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(actuals, preds))\n",
    "        r2 = r2_score(actuals, preds)\n",
    "\n",
    "        results.append([horizon, mae, rmse, r2])\n",
    "\n",
    "    return pd.DataFrame(results, columns=[\"Horizon\", \"MAE\", \"RMSE\", \"R2\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0bc266e2ebb6e95",
   "metadata": {},
   "source": [
    "results = evaluate_multi_step_forecast(model, X_test, Y_test, max_horizon=10)\n",
    "print(results)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
